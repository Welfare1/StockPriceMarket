{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "248e9e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Connexion au cluster MongoDB Atlas\n",
    "client = MongoClient(\"mongodb+srv://amedbah2000:NQerjnFDI1xA8Dc1@cluster0.vbt1opf.mongodb.net/?retryWrites=true&w=majority\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5d052",
   "metadata": {},
   "source": [
    "## Actu et communiqué"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59603b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Scraping AFRICA GLOBAL LOGISTICS (SDSC.ci)...\n",
      "🔍 Scraping AIR LIQUIDE CI (SIVC.ci)...\n",
      "🔍 Scraping BANK OF AFRICA BENIN (BOAB.bj)...\n",
      "🔍 Scraping BANK OF AFRICA BURKINA FASO (BOABF.bf)...\n",
      "🔍 Scraping BANK OF AFRICA CI (BOAC.ci)...\n",
      "🔍 Scraping BANK OF AFRICA MALI (BOAM.ml)...\n",
      "🔍 Scraping BANK OF AFRICA NIGER (BOAN.ne)...\n",
      "🔍 Scraping BANK OF AFRICA SENEGAL (BOAS.sn)...\n",
      "🔍 Scraping BERNABE (BNBC.ci)...\n",
      "🔍 Scraping BICICI (BICC.ci)...\n",
      "🔍 Scraping BRVM - AGRICULTURE (BRVMAG)...\n",
      "🔍 Scraping BRVM - AUTRES SECTEURS (BRVMAS)...\n",
      "🔍 Scraping BRVM - CONSOMMATION DE BASE (BRVM-CB)...\n",
      "🔍 Scraping BRVM - CONSOMMATION DISCRETIONNAIRE (BRVM-CD)...\n",
      "🔍 Scraping BRVM - DISTRIBUTION (BRVMDI)...\n",
      "🔍 Scraping BRVM - ENERGIE (BRVM-EN)...\n",
      "🔍 Scraping BRVM - FINANCE (BRVMFI)...\n",
      "🔍 Scraping BRVM - INDUSTRIE (BRVMIN)...\n",
      "🔍 Scraping BRVM - INDUSTRIELS (BRVM-IN)...\n",
      "🔍 Scraping BRVM - PRESTIGE (BRVMPR)...\n",
      "🔍 Scraping BRVM - PRINCIPAL (BRVMPA)...\n",
      "🔍 Scraping BRVM - SERVICES FINANCIERS (BRVM-SF)...\n",
      "🔍 Scraping BRVM - SERVICES PUBLICS (BRVMSP)...\n",
      "🔍 Scraping BRVM - SERVICES PUBLICS (BRVM-SP)...\n",
      "🔍 Scraping BRVM - TELECOMMUNICATIONS (BRVM-TEL)...\n",
      "🔍 Scraping BRVM - TRANSPORT (BRVMTR)...\n",
      "🔍 Scraping BRVM 30 (BRVM30)...\n",
      "🔍 Scraping BRVM COMPOSITE (BRVMC)...\n",
      "🔍 Scraping Capitalisation BRVM (CAPIBRVM)...\n",
      "🔍 Scraping CFAO CI (CFAC.ci)...\n",
      "🔍 Scraping CIE CI (CIEC.ci)...\n",
      "🔍 Scraping CORIS BANK INTERNATIONAL BF (CBIBF.bf)...\n",
      "🔍 Scraping CROWN SIEM (SEMC.ci)...\n",
      "🔍 Scraping ECOBANK CI (ECOC.ci)...\n",
      "🔍 Scraping ETI TG (ETIT.tg)...\n",
      "🔍 Scraping FILTISAC CI (FTSC.ci)...\n",
      "🔍 Scraping INDICE SIKAFINANCE (SIKAIDX)...\n",
      "🔍 Scraping LOTERIE NATIONALE DU BENIN (LNBB.bj)...\n",
      "🔍 Scraping MOVIS CI (SVOC.ci)...\n",
      "🔍 Scraping NEI CEDA CI (NEIC.ci)...\n",
      "🔍 Scraping NESTLE CI (NTLC.ci)...\n",
      "🔍 Scraping NSIA BANQUE (NSBC.ci)...\n",
      "🔍 Scraping ONATEL BF (ONTBF.bf)...\n",
      "🔍 Scraping ORAGROUP TOGO (ORGT.tg)...\n",
      "🔍 Scraping ORANGE CI (ORAC.ci)...\n",
      "🔍 Scraping PALMCI (PALC.ci)...\n",
      "🔍 Scraping SAFCA CI (SAFC.ci)...\n",
      "🔍 Scraping SAPH CI (SPHC.ci)...\n",
      "🔍 Scraping SERVAIR ABIDJAN CI (ABJC.ci)...\n",
      "🔍 Scraping SETAO CI (STAC.ci)...\n",
      "🔍 Scraping SGBCI (SGBC.ci)...\n",
      "🔍 Scraping SICABLE CI (CABC.ci)...\n",
      "🔍 Scraping SICOR (SICC.ci)...\n",
      "🔍 Scraping SIKA TOTAL RETURN (SIKATR)...\n",
      "🔍 Scraping SITAB (STBC.ci)...\n",
      "🔍 Scraping SMB CI (SMBC.ci)...\n",
      "🔍 Scraping SOCIETE IVOIRIENNE DE BANQUE CI (SIBC.ci)...\n",
      "🔍 Scraping SODECI (SDCC.ci)...\n",
      "🔍 Scraping SOGB (SOGC.ci)...\n",
      "🔍 Scraping SOLIBRA CI (SLBC.ci)...\n",
      "🔍 Scraping SONATEL (SNTS.sn)...\n",
      "🔍 Scraping SUCRIVOIRE (SCRC.ci)...\n",
      "🔍 Scraping TOTAL CI (TTLC.ci)...\n",
      "🔍 Scraping TOTAL SENEGAL (TTLS.sn)...\n",
      "🔍 Scraping TRACTAFRIC MOTORS CI (PRSC.ci)...\n",
      "🔍 Scraping UNILEVER CI (UNLC.ci)...\n",
      "🔍 Scraping UNIWAX CI (UNXC.ci)...\n",
      "🔍 Scraping VIVO ENERGY CI (SHEC.ci)...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "base_url = \"https://www.sikafinance.com\"\n",
    "cot_url = base_url + \"/marches/cotation_\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}\n",
    "\n",
    "# Étape 1 : Récupérer toutes les valeurs du sélecteur\n",
    "homepage = requests.get(cot_url + \"SDSC.ci\", headers=headers)\n",
    "soup_home = BeautifulSoup(homepage.content, \"html.parser\")\n",
    "\n",
    "select = soup_home.select_one(\"#dpShares\")\n",
    "options = select.find_all(\"option\")\n",
    "actions = [(opt.text.strip(), opt[\"value\"].strip()) for opt in options if opt[\"value\"].strip()]\n",
    "\n",
    "\n",
    "all_news = []\n",
    "all_communiques = []\n",
    "\n",
    "for nom_action, valeur in actions:\n",
    "    print(f\"🔍 Scraping {nom_action} ({valeur})...\")\n",
    "\n",
    "    url = cot_url + valeur\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    \n",
    "    # 📰 Actualités\n",
    "    actualites_div = soup.find(\"div\", class_=\"home_content\")\n",
    "    if actualites_div:\n",
    "        spans = actualites_div.find_all(\"span\", class_=\"sp1\")\n",
    "        links = actualites_div.find_all(\"a\", class_=\"lks\")\n",
    "        for span, link in zip(spans, links):\n",
    "            all_news.append({\n",
    "                \"Action\": nom_action,\n",
    "                \"Date\": span.text.strip(),\n",
    "                \"Titre\": link.text.strip(),\n",
    "                \"URL\": base_url + link[\"href\"]\n",
    "            })\n",
    "\n",
    "    # 📑 Communiqués (section suivante sur la page)\n",
    "    communiques_section = actualites_div.find_next(\"div\", class_=\"home_content\")\n",
    "    if communiques_section:\n",
    "        spans = communiques_section.find_all(\"span\", class_=\"sp1\")\n",
    "        links = communiques_section.find_all(\"a\", class_=\"lks\")\n",
    "        for span, link in zip(spans, links):\n",
    "            all_communiques.append({\n",
    "                \"Action\": nom_action,\n",
    "                \"Date\": span.text.strip(),\n",
    "                \"Titre\": link.text.strip(),\n",
    "                \"URL\": base_url + link[\"href\"]\n",
    "            })\n",
    "\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 👉 Résultats\n",
    "df_news = pd.DataFrame(all_news)\n",
    "df_communiques = pd.DataFrame(all_communiques)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75d9250d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Données insérées avec succès dans la collection 'actus_communiques'.\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Sélection de la base et de la collection\n",
    "db = client[\"sika_finance\"]\n",
    "collection = db[\"actus_communiques\"]\n",
    "\n",
    "# 👉 Création des DataFrames\n",
    "df_news = pd.DataFrame(all_news)\n",
    "df_communiques = pd.DataFrame(all_communiques)\n",
    "\n",
    "# Regrouper par action dans df_news\n",
    "actions_uniques = df_news[\"Action\"].unique()\n",
    "\n",
    "# Parcours de chaque action\n",
    "for action in actions_uniques:\n",
    "    # Filtrer les actualités et communiqués pour cette action\n",
    "    actualites = df_news[df_news[\"Action\"] == action].drop(columns=[\"Action\"]).to_dict(orient=\"records\")\n",
    "    communiques = df_communiques[df_communiques[\"Action\"] == action].drop(columns=[\"Action\"]).to_dict(orient=\"records\")\n",
    "\n",
    "    # Supprimer l'existant si besoin\n",
    "    collection.delete_one({\"action\": action})\n",
    "\n",
    "    # Insertion dans MongoDB\n",
    "    collection.insert_one({\n",
    "        \"action\": action,\n",
    "        \"actualites\": actualites,\n",
    "        \"communiques\": communiques\n",
    "    })\n",
    "\n",
    "print(\"✅ Données insérées avec succès dans la collection 'actus_communiques'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6944b694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⌛ Récupération des cotations...\n",
      "✅ AFRICA GLOBAL LOGISTICS mis à jour\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '460,00'\n",
      "✅ BANK OF AFRICA BENIN mis à jour\n",
      "✅ BANK OF AFRICA BURKINA FASO mis à jour\n",
      "✅ BANK OF AFRICA CI mis à jour\n",
      "✅ BANK OF AFRICA MALI mis à jour\n",
      "✅ BANK OF AFRICA NIGER mis à jour\n",
      "✅ BANK OF AFRICA SENEGAL mis à jour\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '865,00'\n",
      "✅ BICICI mis à jour\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '222,91'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '661,74'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '117,46'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '98,55'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '404,35'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '126,51'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '115,18'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '139,29'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '117,67'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '122,03'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '157,18'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '113,20'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '777,90'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '99,60'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '93,97'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '328,58'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '144,66'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '287,19'\n",
      "✅ Capitalisation BRVM mis à jour\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '610,00'\n",
      "✅ CIE CI mis à jour\n",
      "✅ CORIS BANK INTERNATIONAL BF mis à jour\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '700,00'\n",
      "✅ ECOBANK CI mis à jour\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '15,00'\n",
      "✅ FILTISAC CI mis à jour\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '69,08'\n",
      "Erreur extraction cotations: could not convert string to float: '-'\n",
      "✅ MOVIS CI mis à jour\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '620,00'\n",
      "✅ NESTLE CI mis à jour\n",
      "✅ NSIA BANQUE mis à jour\n",
      "✅ ONATEL BF mis à jour\n",
      "✅ ORAGROUP TOGO mis à jour\n",
      "✅ ORANGE CI mis à jour\n",
      "✅ PALMCI mis à jour\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '700,00'\n",
      "✅ SAPH CI mis à jour\n",
      "✅ SERVAIR ABIDJAN CI mis à jour\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '480,00'\n",
      "✅ SGBCI mis à jour\n",
      "✅ SICABLE CI mis à jour\n",
      "✅ SICOR mis à jour\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '277,78'\n",
      "✅ SITAB mis à jour\n",
      "✅ SMB CI mis à jour\n",
      "✅ SOCIETE IVOIRIENNE DE BANQUE CI mis à jour\n",
      "✅ SODECI mis à jour\n",
      "✅ SOGB mis à jour\n",
      "✅ SOLIBRA CI mis à jour\n",
      "✅ SONATEL mis à jour\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '960,00'\n",
      "✅ TOTAL CI mis à jour\n",
      "✅ TOTAL SENEGAL mis à jour\n",
      "✅ TRACTAFRIC MOTORS CI mis à jour\n",
      "✅ UNILEVER CI mis à jour\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '390,00'\n",
      "Erreur extraction cotations: invalid literal for int() with base 10: '925,00'\n",
      "⏳ Pause 15 minutes...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 96\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# Attendre 15 minutes\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⏳ Pause 15 minutes...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 96\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m15\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🕗 Attente de 8h...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Connexion MongoDB\n",
    "db = client[\"sika_finance\"]\n",
    "collection = db[\"cotations\"]\n",
    "\n",
    "def extraire_cotations(soup):\n",
    "    cot1 = soup.find(\"div\", id=\"cot1c\")\n",
    "    if not cot1:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        prix = float(cot1.select_one(\".cot1u\").text.split()[0].replace('\\xa0', '').replace('XOF', '').replace(',', '.'))\n",
    "        variation = cot1.select_one(\".quote_up, .quote_down\").text.strip()\n",
    "        tables = cot1.find_all(\"table\")\n",
    "\n",
    "        values = [td.text.replace('\\xa0', '').strip() for table in tables for td in table.find_all(\"td\")[1::2]]\n",
    "\n",
    "        return {\n",
    "            \"timestamp\": datetime.now(),\n",
    "            \"prix\": prix,\n",
    "            \"variation\": variation,\n",
    "            \"volume_titres\": float(values[0].replace(' ', '').replace(',', '.')),\n",
    "            \"volume_xof\": float(values[1].replace(' ', '').replace(',', '.')),\n",
    "            \"ouverture\": float(values[2].replace(' ', '').replace(',', '.')),\n",
    "            \"plus_haut\": float(values[3].replace(' ', '').replace(',', '.')),\n",
    "            \"plus_bas\": float(values[4].replace(' ', '').replace(',', '.')),\n",
    "            \"cloture_veille\": float(values[5].replace(' ', '').replace(',', '.')),\n",
    "            \"beta\": float(values[6].replace(',', '.')),\n",
    "            \"rsi\": float(values[7].replace(',', '.')),\n",
    "            \"capital_echange\": values[8],\n",
    "            \"valorisation\": values[9]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Erreur extraction cotations:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "base_url = \"https://www.sikafinance.com\"\n",
    "cot_url = base_url + \"/marches/cotation_\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "}\n",
    "\n",
    "# Étape 1 : Récupérer toutes les valeurs du sélecteur\n",
    "homepage = requests.get(cot_url + \"SDSC.ci\", headers=headers)\n",
    "soup_home = BeautifulSoup(homepage.content, \"html.parser\")\n",
    "\n",
    "select = soup_home.select_one(\"#dpShares\")\n",
    "options = select.find_all(\"option\")\n",
    "actions = [(opt.text.strip(), opt[\"value\"].strip()) for opt in options if opt[\"value\"].strip()]\n",
    "\n",
    "# Définir l'heure d'ouverture et de fermeture du marché\n",
    "start = datetime.now().replace(hour=8, minute=0, second=0, microsecond=0)\n",
    "end = datetime.now().replace(hour=18, minute=30, second=0, microsecond=0)\n",
    "\n",
    "# Liste pour stocker les cotations pendant la journée\n",
    "all_cotations = []\n",
    "\n",
    "while datetime.now() < end:\n",
    "    if datetime.now() >= start:\n",
    "        print(\"⌛ Récupération des cotations...\")\n",
    "\n",
    "        # Liste temporaire pour les cotations de cette période\n",
    "        cotations_period = []\n",
    "\n",
    "        for nom_action, valeur in actions:\n",
    "            try:\n",
    "                url = cot_url + valeur\n",
    "                response = requests.get(url, headers=headers)\n",
    "                soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "                cotation = extraire_cotations(soup)\n",
    "                if cotation:\n",
    "                    cotations_period.append({\n",
    "                        \"action\": nom_action,\n",
    "                        **cotation  # Ajoute les cotations extraites à l'entrée\n",
    "                    })\n",
    "                    print(f\"✅ {nom_action} mis à jour\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Erreur lors de la mise à jour de {nom_action}: {e}\")\n",
    "\n",
    "        if cotations_period:\n",
    "            # Ajouter les cotations de la période dans le DataFrame\n",
    "            df = pd.DataFrame(cotations_period)\n",
    "            all_cotations.append(df)\n",
    "        \n",
    "        # Attendre 15 minutes\n",
    "        print(\"⏳ Pause 15 minutes...\")\n",
    "#     \"time.sleep(15 * 60)\n",
    "    else:\n",
    "        print(\"🕗 Attente de 8h...\")\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "130762e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Toutes les cotations ont été insérées dans la base de données.\n"
     ]
    }
   ],
   "source": [
    "# Une fois la période terminée, concaténer les DataFrames collectés\n",
    "if all_cotations:\n",
    "    final_df = pd.concat(all_cotations, ignore_index=True)\n",
    "\n",
    "    # Insérer dans la base de données MongoDB\n",
    "    for _, row in final_df.iterrows():\n",
    "        collection.update_one(\n",
    "            {\"action\": row[\"action\"]},\n",
    "            {\"$push\": {\"cotations\": row.to_dict()}},  # Ajouter chaque cotation à l'action correspondante\n",
    "            upsert=True\n",
    "        )\n",
    "    print(\"✅ Toutes les cotations ont été insérées dans la base de données.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
